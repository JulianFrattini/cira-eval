{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the CiRA Tool\n",
    "\n",
    "This notebook contains the evaluation of CiRA's capability to automatically generate test case descriptions from natural language requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "First, load both the data set of sentences (which also contains the ground truth regarding causality) and the ground truth of test case descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.read_csv('./../data/cwa-acceptance-criteria.csv', usecols=['ID', 'Acceptance Criterion', 'Causal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = './../data/ground-truth/'\n",
    "\n",
    "ground_truth = {}\n",
    "for filename in os.listdir(data_location):\n",
    "    if not filename.startswith('s'):\n",
    "        continue\n",
    "\n",
    "    sentence_id = int(filename.split('.')[0][1:])\n",
    "    \n",
    "    with open(f'{data_location}{filename}') as file:\n",
    "        data = json.load(file)\n",
    "        ground_truth[sentence_id] = data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that CiRA is running locally. If the `api/health` request receives a response resembling `{'status': 'up', 'cira-version': '0.9.4'}` then the tool is available and the evaluation can be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('http://localhost:8000/api/health')\n",
    "response.json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Evaluate CiRA's ability to correctly classify sentences as either causal or non-causal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cira_classification = []\n",
    "for index, row in sentences.iterrows():\n",
    "    sentence = row['Acceptance Criterion']\n",
    "\n",
    "    classification = requests.put('http://localhost:8000/api/classify', \n",
    "                                  data=json.dumps({'sentence': sentence}), \n",
    "                                  headers={'content-type':'application/json'})\n",
    "    cira_classification.append(classification.json()['causal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard = list(sentences['Causal'].values)\n",
    "\n",
    "agreement = 0\n",
    "for index in range(0, len(gold_standard)):\n",
    "    if gold_standard[index] == cira_classification[index]:\n",
    "        agreement += 1\n",
    "\n",
    "print(f'CiRA classified {float(agreement)/len(gold_standard):.2%} of all sentences correctly.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences['CiRA Causal'] = cira_classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case Generation\n",
    "\n",
    "Finally, perform the evaluation of the test case description generation. For this, methods to compare the ground truth test cases have to be defined first before iterating through all causal sentences, generating test cases, and calculating the similarity between the generated test suite and the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variable_by_id(testsuite, id: str) -> dict:\n",
    "    \"\"\"Retrieves a variable from a testsuite dictionary with the given id.\n",
    "    \n",
    "    parameters:\n",
    "      testsuite -- dictionary containing a list of condition variables, a list of expected variables, and a list of combinations\n",
    "      id -- alphanumeric identifier of a variable, typically P plus a number\n",
    "      \n",
    "    returns:\n",
    "      none -- if neither the conditions nor the expected variables contain a variable with the given id\n",
    "      the respective variable -- otherwise\"\"\"\n",
    "    \n",
    "    variables = testsuite['conditions'] + testsuite['expected']\n",
    "\n",
    "    candidates = [variable for variable in variables if variable['id']==id]\n",
    "    if len(candidates) > 0:\n",
    "        return candidates[0]\n",
    "    return None\n",
    "\n",
    "def calculate_variable_similarity(ground_truth: dict, generated: dict) -> float:\n",
    "    \"\"\"Calculate the average similarity of the variables (conditions and expected) between the ground truth and a generated test suite.\n",
    "    \n",
    "    parameters:\n",
    "      ground_truth -- a test suite dictionary generated manually\n",
    "      generated -- a test suite dictionary generated automatically by CiRA\n",
    "      \n",
    "    returns: similarity score between 0 (no match) and 1 (perfect match)\"\"\"\n",
    "\n",
    "    similarity_scores: list[float] = []\n",
    "\n",
    "    for variable in ground_truth['conditions'] + ground_truth['expected']:\n",
    "        variable_generated = get_variable_by_id(generated, variable['id'])\n",
    "\n",
    "        variable_similarity = 0.0\n",
    "        condition_similarity = 0.0\n",
    "        \n",
    "        if variable_generated != None:\n",
    "            variable_similarity = SequenceMatcher(None, variable['variable'], variable_generated['variable']).ratio()\n",
    "            condition_similarity = SequenceMatcher(None, variable['condition'], variable_generated['condition']).ratio()\n",
    "\n",
    "        similarity_scores.append(variable_similarity)\n",
    "        similarity_scores.append(condition_similarity)\n",
    "        \n",
    "    avg_similarity = sum(similarity_scores)/len(similarity_scores)\n",
    "    return avg_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_combinatorics_similarity(ground_truth: dict, generated: dict) -> float: \n",
    "    \"\"\"Calculate the average combinatorics similarity between the ground truth and a generated test suite.\n",
    "    parameters:\n",
    "      ground_truth -- a test suite dictionary generated manually\n",
    "      generated -- a test suite dictionary generated automatically by CiRA\n",
    "      \n",
    "    returns: similarity score between 0 (no match) and 1 (perfect match)\"\"\"\n",
    "    \n",
    "    tcs_manual = ground_truth['cases']\n",
    "    tcs_generated = generated['cases']\n",
    "\n",
    "    similarity_scores: list[float] = []\n",
    "\n",
    "    for index, tc in enumerate(tcs_manual):\n",
    "        similarity = 1.0 if (tc==tcs_generated[index]) else 0.0\n",
    "        similarity_scores.append(similarity)\n",
    "\n",
    "    avg_similarity = sum(similarity_scores)/len(similarity_scores)\n",
    "    return avg_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores_variable: list[float] = []\n",
    "similarity_scores_combinatorics: list[float] = []\n",
    "\n",
    "for index, row in sentences.iterrows():\n",
    "    #if not row['Causal'] or not row['CiRA Causal']:\n",
    "    if not row['Causal'] or index == 42:\n",
    "        continue\n",
    "\n",
    "    sentence = row['Acceptance Criterion']\n",
    "    sentence_id = row['ID']\n",
    "    #print(f'{sentence_id}: {sentence}')\n",
    "\n",
    "    labels = requests.put('http://localhost:8000/api/label', \n",
    "                            data=json.dumps({'sentence': sentence}),\n",
    "                            headers={'content-type':'application/json'}).json()\n",
    "    #print(labels)\n",
    "    \n",
    "    ceg = requests.put('http://localhost:8000/api/graph', \n",
    "                            data=json.dumps({\n",
    "                                'sentence': sentence,\n",
    "                                'labels': labels['labels']\n",
    "                                }),\n",
    "                            headers={'content-type':'application/json'}).json()\n",
    "    #print(ceg)\n",
    "\n",
    "    tests = requests.put('http://localhost:8000/api/testsuite', \n",
    "                            data=json.dumps({\n",
    "                                'sentence': sentence,\n",
    "                                'graph': ceg['graph']\n",
    "                                }),\n",
    "                            headers={'content-type':'application/json'}).json()\n",
    "    #print(tests['suite'])\n",
    "\n",
    "    variable_similarity = calculate_variable_similarity(ground_truth=ground_truth[sentence_id]['testsuite'], generated=tests['suite'])\n",
    "    similarity_scores_variable.append(variable_similarity)\n",
    "\n",
    "    combinatorics_similarity = calculate_combinatorics_similarity(ground_truth=ground_truth[sentence_id]['testsuite'], generated=tests['suite'])\n",
    "    similarity_scores_combinatorics.append(combinatorics_similarity)\n",
    "\n",
    "print(f'Average variable similarity over {len(similarity_scores_variable)} sentences: {sum(similarity_scores_variable)/len(similarity_scores_variable)}')\n",
    "print(f'Average combinatorics similarity over {len(similarity_scores_combinatorics)} sentences: {sum(similarity_scores_combinatorics)/len(similarity_scores_combinatorics)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
